from dotenv import load_dotenv
import os
from langchain_openai import ChatOpenAI
import time
from langchain_core.caches import BaseCache
from typing import Optional, Any
from langchain_core.outputs import Generation
from collections.abc import Sequence
import langchain
from langchain_openai import ChatOpenAI
from langchain.globals import set_llm_cache
from langchain.schema import HumanMessage

RETURN_VAL_TYPE = Sequence[Generation]


load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

class InMemoryCache(BaseCache):
    """Cache that stores things in memory."""

    def __init__(self) -> None:

        self._cache: dict[tuple[str, str], RETURN_VAL_TYPE] = {}
    
    def lookup(self, 
               prompt: str, 
               llm_string: str
               ) -> Optional[RETURN_VAL_TYPE]:

        print(f"âœ…[LOOKUP] prompt={prompt}, llm_string={llm_string}")
        return self._cache.get((prompt, llm_string), None)

    def update(self, 
               prompt: str, 
               llm_string: str, 
               return_val: RETURN_VAL_TYPE) -> None:

        print(f"âŒ[UPDATE] prompt={prompt}, llm_string={llm_string}")
        self._cache[(prompt, llm_string)] = return_val

    def clear(self, **kwargs: Any) -> None:

        self._cache = {}


# å•Ÿç”¨å¿«å–
cache = InMemoryCache()
set_llm_cache(cache)

# åˆå§‹åŒ– LLM
llm = ChatOpenAI(api_key=api_key, max_completion_tokens=50)

# ç”¨çµ±ä¸€çš„ llm_string
llm_string = llm._get_llm_string()
print('--------------------')
print(llm_string)
prompt = [HumanMessage(content="è³½äºè¶…äºº")]

# ç¬¬ä¸€æ¬¡å‘¼å«ï¼šæœƒè§¸ç™¼ lookup(), update()
start_time = time.time()
res = llm.invoke(prompt)
print(res.content)
print("ğŸ”° check: ", llm._get_llm_string())
print("ğŸ”°ğŸ”° check: ", prompt)
print("no cache:", time.time() - start_time)

# ç¬¬äºŒæ¬¡å‘¼å«ï¼šæœƒèµ° lookup(), å› ç‚ºå°æ‡‰åˆ°ç›¸åŒprompt
start_time = time.time()
print(llm.invoke(prompt).content)
print("ğŸ”° check: ", llm._get_llm_string())
print("ğŸ”°ğŸ”° check: ", prompt)
print("cached:", time.time() - start_time)
