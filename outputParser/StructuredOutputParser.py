from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

response_schemas = [

    ResponseSchema(
        name='answer',
        description='æå•æ™‚çš„å›ç­”'
    ),
    ResponseSchema(
        name='source',
        description='å›ç­”çš„ç¶²å€å‡ºè™•'
    )
]

outputparser = StructuredOutputParser(response_schemas=response_schemas)

instructions = outputparser.get_format_instructions()
print(f'instructions: {instructions}')


prompt_tpl = PromptTemplate.from_template(
    template='è«‹å›ç­”ä½¿ç”¨è€…å•é¡Œ.\n{input}\n{instructions}',
    partial_variables={'instructions': instructions}
)

prompt = prompt_tpl.format(input='ä¸­åœ‹æœ‰å¤šå°‘æ°‘æ—')
print('âœ…prompt: ',prompt)

llm = OpenAI(api_key=api_key,max_tokens=150,model_name='gpt-3.5-turbo-instruct')
output = llm.invoke(prompt)
print(f'ğŸ§­output: {output}, ğŸ§­type: {type(output)}')

output_format = outputparser.parse(output)
print(f'ğŸ§©output_format: ')
for k, v in output_format.items():
    print(k, v)